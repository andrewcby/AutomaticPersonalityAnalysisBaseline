{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, csv, random, time\n",
    "\n",
    "from __future__ import division\n",
    "from matplotlib import  pyplot as plt\n",
    "from scipy import stats, optimize\n",
    "from collections import deque\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy.stats import bernoulli,poisson,norm,expon\n",
    "# from pathos.multiprocessing import ProcessingPool as Pool\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pairwise_Newton(GenericLikelihoodModel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Newton's method's last output is NaN for some reason, so we have to collect the second to last result,\n",
    "    which is stored in self.rank[0]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, endog, video_num, video_score, exog=None, **kwds):\n",
    "        super(Pairwise, self).__init__(endog, exog, **kwds)\n",
    "        self.rank =  deque([[1],[1]])\n",
    "        self.w_hat = []\n",
    "        self.video_num = video_num\n",
    "        self.pairs_truth = self.endog\n",
    "        self.total_pairs = len(self.endog)\n",
    "        self.v = []\n",
    "        self.w_star = np.array(video_score)\n",
    "        \n",
    "     \n",
    "    def test_data_generation_sparsity(self, showResult=False, Nihar=False, Isabelle=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Shows how the error evolves with sparser data\n",
    "        \"\"\"\n",
    "        \n",
    "        all_data = self.endog\n",
    "        \n",
    "        num_test_pairs = (np.array((1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1))* self.total_pairs).astype(int)\n",
    "        \n",
    "        output_error_nihar = []\n",
    "        output_error_isabelle = []\n",
    "\n",
    "        for test_pair_num in num_test_pairs:\n",
    "            \n",
    "            print 'Current evaluating with %d test pairs' % test_pair_num\n",
    "            \n",
    "            test_pairs = [self.pairs_truth[i] for i in random.sample(range(self.total_pairs), test_pair_num)]\n",
    "            self.endog = np.array(test_pairs)\n",
    "            \n",
    "            w_init = np.random.uniform(-5,5,video_num)\n",
    "            self.fit(w_init)\n",
    "            \n",
    "            if showResult:\n",
    "                res = self.rank[0]\n",
    "                compare_rank(self.w_star, res, False, True)\n",
    "                plt.legend([str(i) for i in num_test_pairs])  \n",
    "            if Nihar:\n",
    "                output_error_nihar.append(self.performance_nihar())\n",
    "            if Isabelle:\n",
    "                output_error_isabelle.append(self.performance_isabelle())\n",
    "                \n",
    "        self.endog = all_data\n",
    "        return output_error_nihar, output_error_isabelle\n",
    "        \n",
    "        \n",
    "    def test_data_generation_noise(self, showResult=False, Nihar=False, Isabelle=False):\n",
    "        \"\"\"\n",
    "        Generate a decision of whether to flip, using a percentage and then select randomly to flip\n",
    "        \"\"\"\n",
    "        all_data = self.endog\n",
    "\n",
    "        num_test_pairs = self.total_pairs\n",
    "        flip_threshold = np.array((0,0.05,0.1,0.15,0.2,0.25,0.3,0.4,0.5))\n",
    "        \n",
    "        output_error_nihar = []\n",
    "        output_error_isabelle = []\n",
    "\n",
    "        for to_flip in flip_threshold:\n",
    "            \n",
    "            print 'Current evaluating with %f to flip' % to_flip\n",
    "\n",
    "            test_pairs_with_error = self.pairs_truth\n",
    "\n",
    "            num_to_flip = int(num_test_pairs*to_flip)\n",
    "\n",
    "            for i in random.sample(range(num_test_pairs), num_to_flip):\n",
    "                test_pairs_with_error[i] = (self.pairs_truth[i][1],self.pairs_truth[i][0])\n",
    "\n",
    "            self.endog = test_pairs_with_error\n",
    "            \n",
    "            w_init = np.random.uniform(-5,5,video_num)\n",
    "            self.fit(w_init)\n",
    "\n",
    "            if showResult:\n",
    "                res = self.rank[0]\n",
    "                compare_rank(self.w_star, res, False, True)\n",
    "                plt.legend([str(i) for i in num_test_pairs])  \n",
    "            if Nihar:\n",
    "                output_error_nihar.append(self.performance_nihar())\n",
    "            if Isabelle:\n",
    "                output_error_isabelle.append(self.performance_isabelle())\n",
    "                \n",
    "        self.endog = all_data\n",
    "        return output_error_nihar, output_error_isabelle\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def noise_generation(self, pair):\n",
    "        \"\"\"\n",
    "        Generate a decision of whether to flip, using proability from normal distribution and then \n",
    "        Bernoulli to decide whether to flip\n",
    "        \"\"\"\n",
    "\n",
    "        difference = self.rank[0][pair[0]] - self.rank[0][pair[1]]\n",
    "        p = self.sigmoid(difference)*(1-self.sigmoid(difference))\n",
    "        decision = bernoulli.rvs(p,size=1)\n",
    "        \n",
    "        return decision\n",
    "    \n",
    "    \n",
    "    def test_data_generation_noise_generative_model(self, showResult=False, Nihar=False, Isabelle=False):\n",
    "        \"\"\"\n",
    "        Function that calculate error using normal distribution to determine the probility of whether to\n",
    "        flip the pair. Highest chance, at scores equal is around 0.12\n",
    "        \"\"\"\n",
    "        all_data = self.endog\n",
    "\n",
    "        test_pairs = self.pairs_truth\n",
    "        \n",
    "        for i in range(self.total_pairs):\n",
    "            if self.noise_generation(test_pairs[i]):\n",
    "                test_pairs[i][0] = test_pairs[i][1]\n",
    "                test_pairs[i][1] = self.pairs_truth[i][0]\n",
    "                \n",
    "        self.endog = test_pairs\n",
    "        w_init = np.random.uniform(-5,5,video_num)\n",
    "        self.fit(w_init)\n",
    "\n",
    "        if showResult:\n",
    "            res = self.rank[0]\n",
    "            compare_rank(self.w_star, res, False, True)\n",
    "            plt.legend([str(i) for i in num_test_pairs])  \n",
    "        if Nihar:\n",
    "            print self.performance_nihar()\n",
    "        if Isabelle:\n",
    "            print self.performance_isabelle()\n",
    "\n",
    "        self.endog = all_data\n",
    "        \n",
    "    def nloglikeobs(self, params):\n",
    "        \"\"\"\n",
    "        Calculate negative log likelihood used for main optimization\n",
    "        \"\"\"\n",
    "        \n",
    "        out = 1\n",
    "        pairs = self.endog\n",
    "        w = params\n",
    "        \n",
    "        for pair in pairs:\n",
    "            out *= 1/(1+np.exp(w[pair[1]] - w[pair[0]]))   \n",
    "            \n",
    "        return -np.log(out)\n",
    "    \n",
    "    \n",
    "    def score(self, params):\n",
    "        \"\"\"\n",
    "        Calculate gradient\n",
    "        \"\"\"\n",
    "        self.rank.popleft()\n",
    "        w = params\n",
    "        pairs = self.endog\n",
    "        grad = []\n",
    "        for i in range(len(w)):\n",
    "            grad.append(self.calc_gradient(pairs, w, i))\n",
    "        \n",
    "        self.rank.append(grad)\n",
    "        \n",
    "        return np.array(grad)\n",
    "    \n",
    "        \n",
    "    def calc_gradient(self,pairs, w, w_i):\n",
    "        \"\"\"\n",
    "        Function used in score() to calculate gradient\n",
    "        \"\"\"\n",
    "        gradient = 0\n",
    "\n",
    "        for pair in pairs:\n",
    "            if w_i == pair[0]:\n",
    "                out = -1\n",
    "            elif w_i == pair[1]:\n",
    "                out = 1  \n",
    "            else:\n",
    "                out = 0\n",
    "\n",
    "            gradient -= out / (1/(np.exp(w[pair[1]]-w[pair[0]]) + 0.00001) +1 )\n",
    "            \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "    def matching_func(self, param):\n",
    "        \"\"\"\n",
    "        Function used to calculate L2 norm of v and w_star. Used to find a and b.\n",
    "        Here params is [a, b]\n",
    "        \"\"\"\n",
    "        return np.linalg.norm(self.w_star - param[0]*np.array(self.rank[0]) - param[1])\n",
    "\n",
    "    \n",
    "    def regularized_vector(self):\n",
    "        \"\"\"\n",
    "        Function used to generate vector v after using matching_func to find optimal a and b\n",
    "        \"\"\"\n",
    "        coeff = optimize.minimize(self.matching_func, [0, 0])\n",
    "        \n",
    "        self.a = coeff['x'][0]\n",
    "        self.b = coeff['x'][1]\n",
    "        self.v = self.a*np.array(self.rank[0])+self.b\n",
    "        \n",
    "        \n",
    "    def performance_isabelle(self):\n",
    "        \"\"\"\n",
    "        Performance evaluation using method proposed by Isabelle\n",
    "        \"\"\"\n",
    "        epsilon = 0.0001\n",
    "        error = 0\n",
    "        self.regularized_vector()\n",
    "        self.w_hat = self.v\n",
    "    \n",
    "        true_order = np.array(self.w_star).argsort()\n",
    "        \n",
    "        what = [ self.w_hat[i] for i in true_order]\n",
    "        wstar = [ self.w_star[i] for i in true_order]\n",
    "                 \n",
    "        for i in range(len(self.w_star)-2):\n",
    "            error += np.abs((wstar[i+1]-wstar[i])/(wstar[i+2]-wstar[i]+epsilon)-\n",
    "                            (what[i+1]-what[i])/(what[i+2]-what[i]+epsilon))\n",
    "        error /= self.total_pairs-2\n",
    "        return error\n",
    "    \n",
    "\n",
    "    def performance_nihar(self):\n",
    "        \"\"\"\n",
    "        Performance evaluation using method proposed by Nihar\n",
    "        \"\"\"\n",
    "        self.regularized_vector()\n",
    "        return np.linalg.norm(self.w_star - self.v)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=10000, maxfun=50000):  \n",
    "        return super(Pairwise, self).fit(start_params=start_params, \n",
    "                                         method='newton',\n",
    "                                         bounds = (-5,5),\n",
    "                                         maxiter=maxiter,\n",
    "                                         disp=0,\n",
    "                                         maxfun=maxfun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
